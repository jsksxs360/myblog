---
layout: article
title: Keras 深度学习笔记（一）什么是深度学习：人工智能、机器学习以及深度学习
tags:
    - Keras
    - 机器学习
mathjax: true
sidebar:
  nav: keras-note
---

最近几年，人工智能被媒体大肆炒作，我们的未来被渲染成可怕的景象：人类的工作将十分稀少，大部分经济活动都由机器人来完成。但对于机器学习从业者来说，重要的是能够在过度炒作的新闻稿中发现改变世界的重大进展。

本文将介绍关于人工智能、机器学习以及深度学习的背景。

## 人工智能、机器学习与深度学习

人工智能、机器学习与深度学习之间的关系可以用下图来表示：

<img src="/img/article/keras-note-1/ai.png" width="350px" style="display: block; margin: auto;">

### 人工智能

人工智能诞生于 20 世纪 50 年代，当时少数计算机科学的先驱开始探索“计算机是否能够思考”的问题。人工智能的简洁定义如下：**努力将通常由人类完成的智力任务自动化**。因此，人工智能是一个综合性的领域，不仅包括机器学习与深度学习，还包括更多不涉及学习的方法。

从 20 世纪 50 年代到 80 年代末，许多专家相信只要人工精心编写足够多的规则来处理知识，就可以实现与人类水平相当的人工智能，这一方法被称为**符号主义人工智能** (symbolic AI)。这一方法热度的顶峰就是 20 世纪 80 年代的**专家系统** (expert system)。

虽然符号主义人工智能能够解决定义明确的逻辑问题，但它难以解决更加复杂、模糊的问题，比如图像分类、语音识别和语言翻译。于是出现了一种新的方法来替代符号主义人工智能，这就是**机器学习** (machine learning)。

### 机器学习

在维多利亚时代的英格兰，查尔斯 • 巴贝奇发明了**分析机** (Analytical Engine)，即第一台通用的机械式计算机。1843 年，他的好友兼合作者埃达 • 洛夫莱斯伯爵夫人对这项发明评论道：“分析机谈不上能创造什么东西。它只能完成我们命令它做的任何事情……它的职责是帮助我们去实现我们已知的事情。”1950 年，阿兰 • 图灵在《计算机器和智能》中将上述评论称为“洛夫莱斯伯爵夫人的异议”，并且思考了这样一个问题：通用计算机是否能够学习与创新？他得出的结论是“能”。

机器学习的概念就来自于图灵的这个问题：计算机能否自我学习执行特定任务的方法？如果没有程序员精心编写的数据处理规则，计算机能否通过观察数据自动学会这些规则？

在经典的程序设计中，人们输入的是规则（即程序）和需要根据这些规则进行处理的数据，系统输出的是答案。而机器学习，人们输入的是数据和从这些数据中预期得到的答案，系统输出的是规则。这些规则随后可应用于新的数据，并使计算机自主生成答案。

<img src="/img/article/keras-note-1/machine_learning.png" width="400px" style="display: block; margin: auto;">

机器学习系统是**训练出来的**，而不是明确地用程序编写出来的。将与某个任务相关的许多示例输入机器学习系统，它会在这些示例中找到统计结构，从而最终找到规则将任务自动化。

机器学习在 20 世纪 90 年代才开始发展，得益于速度更快的硬件与更大的数据集，它迅速成为人工智能最受欢迎且最成功的分支领域。机器学习与数理统计密切相关但又有所不同：不同于统计学，机器学习经常用于处理复杂的大型数据集（比如包含数百万张图像的数据集，每张图像又包含数万个像素），用经典的统计分析（比如贝叶斯分析）来处理这种数据集是不切实际的。因此，机器学习（尤其是深度学习）呈现出相对较少的数学理论，并且是以工程为导向的，想法更多地是靠实践来证明，而不是理论推导。

### 从数据中学习表示

给定包含预期结果的示例，机器学习会从中发现规则，因此机器学习包含三个要素：

- **输入数据点**。例如，你的任务是为图像添加标签，那么这些数据点可能是图像。
- **预期输出的示例**。对于图像标记任务来说，预期输出可能是“狗”“猫”之类的标签。
- **衡量算法效果好坏的方法**。这一衡量方法是为了计算算法的当前输出与预期输出的差距。衡量结果是一种反馈信号，用于调节算法的工作方式。这个调节步骤就是我们所说的**学习**。

机器学习是一个从已知的输入和输出示例中进行“学习”的过程，它的核心问题在于**有意义地变换数据**，即在于学习输入数据的有用**表示** (representation)——这种表示可以让数据更接近预期输出。在处理某些任务时，使用某种表示可能会很困难，但换用另一种表示就会变得很简单。机器学习模型都是为输入数据寻找合适的表示，使其更适合手头的任务（比如分类）。

> 例如，彩色图像可以编码为 RGB (红-绿-蓝) 格式或 HSV (色相-饱和度-明度) 格式。对于“选择图像中所有红色像素”这个任务，使用 RGB 格式会更简单，而对于“降低图像饱和度”这个任务，使用 HSV 格式则更简单。

考虑坐标系中的一些白点和一些黑点，假设我们想要开发一个算法，输入一个点的坐标 $(x,y)$，就能够判断这个点是黑色还是白色。

<img src="/img/article/keras-note-1/coordinate_system.png" width="200px" style="display: block; margin: auto;">

在这个例子中：

- 输入是点的坐标；
- 预期输出是点的颜色；
- 衡量算法效果好坏的一种方法是，正确分类的点所占的百分比。

这里我们需要的是一种新的数据表示，可以明确区分白点与黑点。以坐标变换方法为例：

<img src="/img/article/keras-note-1/coordinate_system_2.png" width="800px" style="display: block; margin: auto;">

在这个新的坐标系中，坐标可以看作数据的一种新的表示。利用这种新的表示，用一条简单的规则“$x>0$ 的是黑点”或“$x<0$ 的是白点”就可以完成分类。

上面我们是人为定义了坐标变换，而如果我们尝试系统性地搜索各种可能的坐标变换，并用正确分类的点所占百分比作为反馈信号，那么这就是机器学习。机器学习中的**学习**指的是寻找更好数据表示的自动搜索过程。

所有机器学习算法都在自动寻找可以根据任务将数据转化为更加有用表示的变换，这些操作可能是坐标变换、线性投影（可能会破坏信息）、平移、非线性操作等等。机器学习算法在寻找这些变换时通常仅仅是遍历一组预先定义好的操作，这组操作叫作**假设空间** (hypothesis space)。

这就是机器学习的技术定义：**在预先定义好的假设空间中，利用反馈信号的指引来寻找输入数据的有用表示。**

### 深度学习之“深度”

深度学习是机器学习的一个分支领域：它强调从连续的**层** (layer) 中进行学习，这些层对应于越来越有意义的表示。数据模型的**深度** (depth) 指的是模型中层的数量。

> 现代深度学习通常包含数十个甚至上百个连续的表示层，这些表示层全都是从训练数据中自动学习的。而机器学习方法的重点往往是仅仅学习一两层的数据表示，因此有时也被称为**浅层学习** (shallow learning)。

这些分层表示通过**神经网络** (neural network) 模型来学习得到，神经网络的结构是逐层堆叠。这一术语来自于神经生物学，然而，虽然深度学习的一些核心概念是从人们对大脑的理解中汲取部分灵感而形成的，但深度学习模型不是大脑模型，它只是从数据中学习表示的一种数学框架。

> 一些文章宣称深度学习的工作原理与大脑相似或者是根据大脑的工作原理进行建模的，但事实并非如此。如果认为深度学习与神经生物学存在任何关系，反而会使人困惑。

深度学习算法学到的表示是什么样的？我们来看一个多层网络如何对数字图像进行变换，以便识别图像中所包含的数字。

<img src="/img/article/keras-note-1/multi_layer_network.png" width="600px" style="display: block; margin: auto;">

如下图所示，这个网络将数字图像转换成与原始图像差别越来越大的表示，而其中关于最终结果的信息却越来越丰富。你可以将深度网络看作多级信息蒸馏操作：信息穿过连续的过滤器，**纯度**越来越高（即对任务的帮助越来越大）。

<img src="/img/article/keras-note-1/deep_learning.png" width="600px" style="display: block; margin: auto;">

这就是深度学习的技术定义：学习数据表示的多级方法。这个想法很简单，但事实证明，非常简单的机制如果具有足够大的规模，将会产生魔法般的效果。

### 用三张图理解深度学习的工作原理

深度神经网络通过一系列简单的数据变换（层）来实现从输入（比如图像）到目标（比如标签“猫”）的映射，而这些数据变换是通过观察许多输入和目标的示例来完成的。下面来具体看一下这种学习过程是如何发生的。

神经网络中每层实现的变换由其**权重**来参数化 (parameterize，见下图)，权重有时也被称为该层的**参数** (parameter)。因而**学习**就是为神经网络的所有层找到一组权重值，使得该网络能够将每个示例输入与其目标正确地一一对应。但是一个深度神经网络可能包含数千万个参数，找到所有参数的正确取值可能是一项非常艰巨的任务，特别是考虑到修改某个参数值将会影响其他所有参数的行为。

<img src="/img/article/keras-note-1/deep_learning_2.png" width="500px" style="display: block; margin: auto;">

想要控制神经网络的输出，就需要能够衡量该输出与预期值之间的距离，这就是神经网络**损失函数** (loss function) 也叫**目标函数** (objective function) 的任务。损失函数的输入是网络预测值与真实目标值，然后计算一个距离值，衡量该网络在这个示例上的效果好坏，如下图所示。

<img src="/img/article/keras-note-1/loss_function.png" width="500px" style="display: block; margin: auto;">

深度学习的基本技巧是利用这个距离值作为反馈信号来对权重值进行微调，以降低当前示例对应的损失值（见下图）。这种调节由**优化器** (optimizer) 来完成，它实现了所谓的**反向传播** (backpropagation) 算法，这是深度学习的核心算法。下一篇《XXX》中我们会详细地解释反向传播的工作原理。

<img src="/img/article/keras-note-1/backpropagation.png" width="500px" style="display: block; margin: auto;">

一开始对神经网络的权重随机赋值，因此输出结果和理想值相去甚远，损失值也很高。随着网络处理的示例越来越多，权重值向正确的方向逐步微调，损失值也逐渐降低。这就是**训练循环** (training loop)，将这种循环重复足够多的次数（通常对数千个示例进行数十次迭代），得到的权重值可以使损失函数最小。具有最小损失的网络，其输出值与目标值尽可能地接近，这就是训练好的网络。

再次强调，这是一个简单的机制，一旦具有足够大的规模，将会产生魔法般的效果。

### 深度学习已经取得的进展

虽然深度学习提出很早，但直到 21 世纪的前十年才引起人们的重视。在随后的几年里，它在实践中取得了革命性进展，特别是在视觉和听觉等这些过去机器难以解决的感知问题上取得了令人瞩目的成果。例如：

- 接近人类水平的图像分类
- 接近人类水平的语音识别
- 接近人类水平的手写文字转录
- 更好的机器翻译
- 更好的文本到语音转换
- 数字助理，比如谷歌即时（Google Now）和亚马逊 Alexa
- 接近人类水平的自动驾驶
- 更好的广告定向投放，Google、百度、必应都在使用
- 更好的网络搜索结果
- 能够回答用自然语言提出的问题
- 在围棋上战胜人类

我们仍然在探索深度学习能力的边界。我们已经开始将其应用于机器感知和自然语言理解之外的各种问题，比如形式推理。如果能够成功的话，这可能预示着深度学习将能够协助人类进行科学研究、软件开发等活动。

## 深度学习之前：机器学习简史

虽然深度学习已经得到了前所未有的公众关注度和产业投资，但是当前工业界所使用的绝大部分机器学习算法都不是深度学习算法。深度学习不一定总是解决问题的正确工具：有时没有足够的数据，深度学习不适用；有时用其他算法可以更好地解决问题。

下面我们将简要回顾经典的机器学习方法，并介绍这些方法的历史背景，以便更好地理解深度学习的起源以及它为什么如此重要。

### 概率建模

**概率建模** (probabilistic modeling) 是统计学原理在数据分析中的应用。它是最早的机器学习形式之一，至今仍在广泛使用。

其中最有名的算法之一就是**朴素贝叶斯**算法。朴素贝叶斯是一类基于应用贝叶斯定理的机器学习分类器，它假设输入数据的特征都是独立的。这种数据分析方法比计算机出现得还要早，在其第一次被计算机实现的几十年前就已经靠人工计算来应用了。

另一个密切相关的模型是 **logistic 回归** (logistic regression，简称 logreg)，不要被它的名称所误导——logreg 是一种分类算法，而不是回归算法。与朴素贝叶斯类似，logreg 的出现也比计算机早很长时间，但由于它既简单又通用，至今仍然很有用。面对一个数据集，数据科学家通常会首先尝试使用这个算法，以便初步熟悉手头的分类任务。

### 早期神经网络

早在 20 世纪 50 年代人们就将神经网络作为玩具项目，并对其核心思想进行研究，但在很长一段时间内，一直没有训练大型神经网络的有效方法。一直到 20 世纪 80 年代中期，当时很多人都独立地重新发现了反向传播算法——一种利用梯度下降优化来训练一系列参数化运算链的方法，并开始将其应用于神经网络。

> 贝尔实验室于 1989 年第一次成功实现了神经网络的实践应用，当时 Yann LeCun 将卷积神经网络与反向传播算法相结合，并将其应用于手写数字分类问题，由此得到名为 **LeNet** 的网络，在 20 世纪 90 年代被美国邮政署采用，用于自动读取信封上的邮政编码。

### 核方法

20 世纪 90 年代神经网络开始在研究人员中受到一定的重视，但一种新的机器学习方法很快就抢了神经网络的风头，这种方法就是**核方法** (kernel method)。核方法是一组分类算法，其中最有名的就是**支持向量机** (SVM，support vector machine)。

SVM 的目标是通过在两个不同类别的数据点之间找到良好的**决策边界** (decision boundary，如下图) 来解决分类问题。决策边界可以看作一条直线或一个平面，将训练数据划分为两块空间，分别对应于两个类别。对于新数据点的分类，你只需判断它位于决策边界的哪一侧。

<img src="/img/article/keras-note-1/decision_boundary.png" width="150px" style="display: block; margin: auto;">

SVM 通过两步来寻找决策边界：

1. 将数据映射到一个新的高维表示，这时决策边界可以用一个超平面来表示。
2. 尽量让超平面与每个类别最近的数据点之间的距离最大化，从而计算出良好决策边界（分割超平面），这一步叫作**间隔最大化** (maximizing the margin)。这样决策边界可以很好地推广到训练数据集之外的新样本。

将数据映射到高维表示从而使分类问题简化听起来很不错，但在实践中通常是难以计算的。这时就需要用到**核技巧** (kernel trick)。其基本思想是：要想在新的表示空间中找到良好的决策超平面，你不需要在新空间中直接计算点的坐标，只需要在新空间中计算点对之间的距离，而利用**核函数** (kernel function) 可以高效地完成这种计算。核函数通常是人为选择的——对于 SVM 来说，只有分割超平面是通过学习得到的。

> 核函数能够将原始空间中的任意两点映射为这两点在目标表示空间中的距离，完全避免了对新表示进行直接计算。

SVM 刚刚出现时，在简单的分类问题上表现出了最好的性能，并且其得到数学理论的支持，非常易于理解和解释，因此在很长一段时间里它在实践中非常流行。但是，SVM 很难扩展到大型数据集，在图像分类等感知问题上的效果也不好。SVM 是一种比较浅层的方法，因此要想将其应用于感知问题，首先需要手动提取出有用的表示（这叫作**特征工程**），这一步骤很难，而且不稳定。

### 决策树、随机森林与梯度提升机

**决策树** (decision tree) 是类似于流程图的结构，可以对输入数据点进行分类或根据给定输入来预测输出值（见下图）。决策树的可视化和解释都很简单。在 21 世纪前十年，从数据中学习得到的决策树开始引起研究人员的广泛关注。到了 2010 年，决策树经常比核方法更受欢迎。

<img src="/img/article/keras-note-1/decision_tree.png" width="400px" style="display: block; margin: auto;">

特别是**随机森林** (random forest) 算法，它引入了一种健壮且实用的决策树学习方法，即首先构建许多决策树，然后将它们的输出集成在一起。随机森林适用于各种各样的问题——对于任何浅层的机器学习任务来说，它几乎总是第二好的算法。

> 广受欢迎的机器学习竞赛网站 Kaggle 在 2010 年上线后，随机森林迅速成为平台上人们的最爱，直到 2014 年才被梯度提升机所取代。

与随机森林类似，**梯度提升机** (gradient boosting machine) 也是将弱预测模型（通常是决策树）集成的机器学习技术，并且在绝大多数情况下效果都比随机森林要好。它使用了**梯度提升方法**，通过迭代地训练新模型来专门解决之前模型的弱点，从而改进任何机器学习模型的效果。它可能是目前处理非感知数据最好的算法之一，也是 Kaggle 竞赛中最常用的技术之一。

### 回到神经网络

虽然神经网络几乎被整个科学界完全忽略，但仍有一些人在继续研究神经网络，并在 2010 年左右开始取得重大突破。这些人包括：多伦多大学 Geoffrey Hinton 的小组、蒙特利尔大学的 Yoshua Bengio、纽约大学的 Yann LeCun 和瑞士的 IDSIA。

2012 年深度学习在图像领域初露锋芒，由 Alex Krizhevsky 带领并由 Geoffrey Hinton 提供建议的小组参加了每年一次的大规模图像分类挑战赛 ImageNet，实现了 83.6% 的 top-5 精度，而 2011 年获胜的基于经典的计算机视觉方法的模型 top-5 精度只有 74.3%。此后，这项竞赛每年都由深度卷积神经网络所主导。到了 2015 年，获胜者的精度达到了 96.4%，ImageNet 的分类任务被认为是一个已经完全解决的问题。

> 大规模图像分类挑战赛 ImageNet 以困难著称，参赛者需要对 140 万张高分辨率彩色图像进行训练，然后将其划分到 1000 个不同的类别中。

自 2012 年以来，深度卷积神经网络 (convnet) 已成为所有计算机视觉任务的首选算法。更一般地说，它在所有感知任务上都有效，比如自然语言处理。在大量应用中深度神经网络完全取代了 SVM 与决策树。

> 欧洲核子研究中心 (CERN) 多年来一直使用基于决策树的方法来分析来自大型强子对撞机 (LHC) ATLAS 探测器的粒子数据，但 CERN 最终转向基于 Keras 的深度神经网络，因为它的性能更好，而且在大型数据集上易于训练。

### 深度学习有何不同

深度学习发展得如此迅速，主要原因在于它在很多问题上都表现出更好的性能。而且深度学习让解决问题变得更加简单，因为它将特征工程完全自动化，而这曾经是机器学习工作流程中最关键的一步。

先前的机器学习技术（浅层学习）使用简单的变换（比如高维非线性投影或决策树）仅包含将输入数据变换到一两个连续的表示空间，通常无法得到复杂问题所需要的精确表示。因此，人们必须让初始输入数据更适合用这些方法处理，也必须手动为数据设计好的表示层，这叫作**特征工程**。而深度学习可以一次性自动学习所有特征，无须手动设计，极大地简化了机器学习工作流程，将复杂的多阶段流程替换为一个简单的、端到端的深度学习模型。

那么能否重复应用浅层方法，以实现和深度学习类似的效果？在实践中，如果连续应用浅层学习方法，其收益会随着层数增加迅速降低。深度学习的变革性在于，模型可以在同一时间**共同**学习所有表示层，而不是依次连续学习（这被称为**贪婪**学习）。通过共同的特征学习，一旦模型修改某个内部特征，所有依赖于该特征的其他特征都会相应地自动调节适应。一切都由单一反馈信号来监督，这种方法比贪婪地叠加浅层模型更加强大，因为它可以通过将复杂、抽象的表示拆解为很多个中间层来学习这些表示，每个中间层仅仅是前一个层的简单变换。

深度学习从数据中进行学习时有两个基本特征，使得深度学习比先前的机器学习方法更加成功：

- 通过渐进的、逐层的方式形成越来越复杂的表示；
- 对中间这些渐进的表示共同进行学习，每一层的变化都需要同时考虑上下两层的需要。

### 机器学习现状

要想了解机器学习算法和工具的现状，一个好方法是看一下 Kaggle 上的机器学习竞赛。

在 2016 年和 2017 年，Kaggle 上主要有两大方法：梯度提升机和深度学习。具体而言，梯度提升机用于处理结构化数据的问题，而深度学习则用于图像分类等感知问题。使用前一种方法的人几乎都使用优秀的 XGBoost 库，它同时支持数据科学最流行的两种语言：Python 和 R。使用深度学习的 Kaggle 参赛者则大多使用 Keras 库，因为它易于使用，非常灵活，并且支持 Python。

要想在如今的应用机器学习中取得成功，你应该熟悉这两种技术：梯度提升机，用于浅层学习问题；深度学习，用于感知问题。

> 该文是[《Python深度学习》](http://www.ituring.com.cn/book/2599)的阅读笔记，内容摘自原书，部分内容有修改。